{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors:\n",
    "- Jai Deshmukh, Logan Um, Jackson Burns, Joseph Glennon, Mitchell Kuhns\n",
    "- Team: Virginia Tech S24-37, Battle Damage Indicator\n",
    "- Sponsor: Naval Warfare Surface Center Dahlgren Division\n",
    "\n",
    "## File info:\n",
    "- File name: vt24-37_model_test.ipynb\n",
    "- File info: This file contains a CNN model trained on converted mel spectrograms generated from the preprocessing files associated with this project.\n",
    "- This jupyter notebook uses data files from this paper: https://zenodo.org/records/7779574\n",
    "\n",
    "## Contact:\n",
    "Please contact one of the team members of Virginia Tech S24_37. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "import tensorflow as tf\n",
    "import matplotlib.cm as cm\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from IPython.display import Image, display\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions for code below\n",
    "1. Change height and width to the spectrogram's height and width. This can be collected through right clicking on a spectrogram image and finding the height and width in pixels.\n",
    "2. Change batch_size as desired.\n",
    "3. Change the train_generator and test_generator's directory parameter to the testing and training. The testing and training directories should contain subfolders which each their respective dataset. For example, drones with damage MF1 (motor failure 1) would be in the subfolder MF1. Using the data mentioned above, there should be 9 classes.\n",
    "4. Ensure that running the code below outputs the correct number of images with the correct number of classes.\n",
    "5. Change class_mode parameter to 'binary' or 'categorical' based on the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2324 images belonging to 9 classes.\n",
      "Found 2293 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "height = 138\n",
    "width = 1108\n",
    "img_size = (height, width)\n",
    "batch_size = 128\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    './B/train/wavs', # dataset for training\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    './B/test/wavs', # dataset for testing\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions for CNN model\n",
    "1. Chnage num_classes to number of classes\n",
    "2. You may change model to have less/more layers.\n",
    "3. If desired, the optimizer for the model can be changed.\n",
    "4. Run the code and ensure training has begun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 4s/step - accuracy: 0.3321 - loss: 4.9542 - val_accuracy: 0.1321 - val_loss: 2.5037\n",
      "Epoch 2/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 4s/step - accuracy: 0.4240 - loss: 1.7805 - val_accuracy: 0.1321 - val_loss: 2.5110\n",
      "Epoch 3/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 4s/step - accuracy: 0.4260 - loss: 1.7446 - val_accuracy: 0.1321 - val_loss: 2.4003\n",
      "Epoch 4/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 4s/step - accuracy: 0.4311 - loss: 1.7406 - val_accuracy: 0.1321 - val_loss: 2.5763\n",
      "Epoch 5/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 4s/step - accuracy: 0.4228 - loss: 1.7628 - val_accuracy: 0.1321 - val_loss: 2.4711\n",
      "Epoch 6/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 4s/step - accuracy: 0.4284 - loss: 1.7437 - val_accuracy: 0.1321 - val_loss: 2.6475\n",
      "Epoch 7/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 4s/step - accuracy: 0.4268 - loss: 1.7398 - val_accuracy: 0.1321 - val_loss: 2.5018\n",
      "Epoch 8/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 4s/step - accuracy: 0.4296 - loss: 1.7390 - val_accuracy: 0.1321 - val_loss: 2.5241\n",
      "Epoch 9/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 4s/step - accuracy: 0.4494 - loss: 1.7177 - val_accuracy: 0.1321 - val_loss: 2.6125\n",
      "Epoch 10/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 4s/step - accuracy: 0.4458 - loss: 1.7030 - val_accuracy: 0.1321 - val_loss: 2.5050\n",
      "Epoch 11/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 4s/step - accuracy: 0.4458 - loss: 1.7272 - val_accuracy: 0.1321 - val_loss: 2.5250\n",
      "Epoch 12/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 4s/step - accuracy: 0.4257 - loss: 1.7447 - val_accuracy: 0.1321 - val_loss: 2.5765\n",
      "Epoch 13/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 4s/step - accuracy: 0.4255 - loss: 1.7509 - val_accuracy: 0.1321 - val_loss: 2.5655\n",
      "Epoch 14/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 4s/step - accuracy: 0.4161 - loss: 1.7451 - val_accuracy: 0.1321 - val_loss: 2.4981\n",
      "Epoch 15/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 4s/step - accuracy: 0.4425 - loss: 1.7248 - val_accuracy: 0.1321 - val_loss: 2.3160\n",
      "Epoch 16/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 4s/step - accuracy: 0.4244 - loss: 1.7059 - val_accuracy: 0.1321 - val_loss: 2.2065\n",
      "Epoch 17/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 4s/step - accuracy: 0.4291 - loss: 1.7038 - val_accuracy: 0.1321 - val_loss: 2.2311\n",
      "Epoch 18/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 4s/step - accuracy: 0.4322 - loss: 1.6811 - val_accuracy: 0.1352 - val_loss: 2.6498\n",
      "Epoch 19/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 4s/step - accuracy: 0.4319 - loss: 1.6941 - val_accuracy: 0.2547 - val_loss: 2.1766\n",
      "Epoch 20/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 4s/step - accuracy: 0.4356 - loss: 1.6810 - val_accuracy: 0.2569 - val_loss: 2.3008\n",
      "Epoch 21/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 4s/step - accuracy: 0.4401 - loss: 1.6570 - val_accuracy: 0.2246 - val_loss: 2.9515\n",
      "Epoch 22/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 4s/step - accuracy: 0.4226 - loss: 1.7109 - val_accuracy: 0.2516 - val_loss: 2.0707\n",
      "Epoch 23/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 4s/step - accuracy: 0.4270 - loss: 1.6680 - val_accuracy: 0.2543 - val_loss: 2.2137\n",
      "Epoch 24/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 4s/step - accuracy: 0.4444 - loss: 1.6247 - val_accuracy: 0.2625 - val_loss: 2.5654\n",
      "Epoch 25/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 4s/step - accuracy: 0.4474 - loss: 1.6178 - val_accuracy: 0.2604 - val_loss: 2.7124\n",
      "Epoch 26/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 4s/step - accuracy: 0.4545 - loss: 1.5974 - val_accuracy: 0.2573 - val_loss: 3.0694\n",
      "Epoch 27/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 4s/step - accuracy: 0.4471 - loss: 1.5998 - val_accuracy: 0.2625 - val_loss: 2.3675\n",
      "Epoch 28/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 4s/step - accuracy: 0.4646 - loss: 1.5773 - val_accuracy: 0.2625 - val_loss: 2.1559\n",
      "Epoch 29/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 4s/step - accuracy: 0.4633 - loss: 1.6084 - val_accuracy: 0.2617 - val_loss: 2.1077\n",
      "Epoch 30/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 4s/step - accuracy: 0.4676 - loss: 1.5387 - val_accuracy: 0.3201 - val_loss: 2.0308\n",
      "Epoch 31/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 4s/step - accuracy: 0.4563 - loss: 1.5716 - val_accuracy: 0.2634 - val_loss: 2.0776\n",
      "Epoch 32/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 4s/step - accuracy: 0.4642 - loss: 1.5502 - val_accuracy: 0.3437 - val_loss: 2.2874\n",
      "Epoch 33/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 4s/step - accuracy: 0.4642 - loss: 1.5402 - val_accuracy: 0.2848 - val_loss: 2.1056\n",
      "Epoch 34/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 4s/step - accuracy: 0.4437 - loss: 1.5856 - val_accuracy: 0.2935 - val_loss: 1.9785\n",
      "Epoch 35/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 4s/step - accuracy: 0.4585 - loss: 1.5561 - val_accuracy: 0.2948 - val_loss: 2.1270\n",
      "Epoch 36/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 4s/step - accuracy: 0.4670 - loss: 1.5476 - val_accuracy: 0.3397 - val_loss: 2.2155\n",
      "Epoch 37/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 4s/step - accuracy: 0.4605 - loss: 1.5127 - val_accuracy: 0.3371 - val_loss: 2.5132\n",
      "Epoch 38/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 4s/step - accuracy: 0.4615 - loss: 1.5292 - val_accuracy: 0.3737 - val_loss: 1.9060\n",
      "Epoch 39/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 4s/step - accuracy: 0.4790 - loss: 1.4922 - val_accuracy: 0.3136 - val_loss: 2.4570\n",
      "Epoch 40/40\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 4s/step - accuracy: 0.4551 - loss: 1.5200 - val_accuracy: 0.3615 - val_loss: 2.0009\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 746ms/step - accuracy: 0.3618 - loss: 1.9595\n",
      "Test accuracy: 0.36153510212898254\n"
     ]
    }
   ],
   "source": [
    "num_classes = 9  # Change this to the number of classes in your dataset\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(height, width, 3)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(num_classes, activation='softmax') \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Ensure your train_generator and test_generator yield data and labels in categorical format\n",
    "model.fit(train_generator, epochs=40, validation_data=test_generator, batch_size=batch_size)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
